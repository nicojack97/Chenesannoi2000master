{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Chatbot.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyNGaxmagfhs9gkXCFTvrA0v",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nicojack97/Chenesannoi2000master/blob/master/Chatbot.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Oje-KwCqGSS9",
        "outputId": "77fd1c0c-5fb8-432f-99ca-bf69bf0f73dc"
      },
      "source": [
        "\r\n",
        "# The following application is a simple chatbot, called Friday, which is able to respond to a series of simple\r\n",
        "# questions, using as training set a json file containing sample sentences about 19 topics.\r\n",
        "# The pre-processing of the sentences ha been done using the nltk module for natural language manipulation:\r\n",
        "# the sentences has been first tokenized and then stemmed with the Lancaster stemmer\r\n",
        "# The model used is a 2 layer, fully connected dense neural network first the model is trained on the json file data\r\n",
        "# and then it is used to classify the sentence coming from the input of the user, and respond accordingly.\r\n",
        "## GIACOMO PERONI\r\n",
        "\r\n"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: tflearn in /usr/local/lib/python3.6/dist-packages (0.5.0)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.6/dist-packages (from tflearn) (7.0.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from tflearn) (1.15.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from tflearn) (1.19.4)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MnlAtuSH3DAa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cc20d7af-022a-4f1b-b52e-277ec38bd45a"
      },
      "source": [
        "\r\n",
        "!pip install tflearn\r\n",
        "import nltk\r\n",
        "from nltk.stem.lancaster import LancasterStemmer\r\n",
        "stemmer = LancasterStemmer()\r\n",
        "nltk.download('punkt')\r\n",
        "import tensorflow as tf\r\n",
        "import numpy as np\r\n",
        "import tflearn\r\n",
        "import random\r\n",
        "import json\r\n",
        "\r\n",
        "with open('intents.json') as file:\r\n",
        "  data = json.load(file)"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/compat/v2_compat.py:96: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "non-resource variables are not supported in the long term\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iXDCkdonc2ew",
        "outputId": "9d29f7cb-d429-4097-cc33-446c69213abe"
      },
      "source": [
        "  ## DATA PREPROCESSING:\r\n",
        "  \r\n",
        "  words = []\r\n",
        "  labels = []\r\n",
        "  docs_x = []\r\n",
        "  docs_y =[]\r\n",
        "\r\n",
        "  for intent in data['intents']:\r\n",
        "    for pattern in intent['patterns']:\r\n",
        "      ## the tokenizing process breaks the sentences into single words\r\n",
        "      wrds = nltk.word_tokenize(pattern)\r\n",
        "      words.extend(wrds)\r\n",
        "      docs_x.append(wrds)\r\n",
        "      docs_y.append(intent['tag'])\r\n",
        "\r\n",
        "    if intent['tag'] not in labels:\r\n",
        "      labels.append(intent['tag'])\r\n",
        "\r\n",
        "  ## the stemming process extracts the root/base form of the worlds e.g goes, gone, go --> go\r\n",
        "  words = [stemmer.stem(w.lower()) for w in words if w not in '?']\r\n",
        "  words = sorted(list(set(words)))\r\n",
        "  ##print(words)\r\n",
        "\r\n",
        "  labels=sorted(labels)\r\n",
        "\r\n",
        "  ## preparing the sets needed for the model\r\n",
        "  training = []\r\n",
        "  output = []\r\n",
        "  out_empty = [0 for _ in range(len(labels))]\r\n",
        "\r\n",
        "  ## one hot encoding: translate every sentence into an array as long as the number of\r\n",
        "  ## words and having a 1 if the word at the i's index is contained in the sentence and 0 elsewhere \r\n",
        "  for x,doc in enumerate(docs_x):\r\n",
        "      bag= []\r\n",
        "    \r\n",
        "      wrds = [stemmer.stem(w) for w in doc]\r\n",
        "      for w in words:\r\n",
        "          if w in wrds:\r\n",
        "            bag.append(1)\r\n",
        "          else:\r\n",
        "            bag.append(0)\r\n",
        "\r\n",
        "##the output simply tells us for every sentence, what category (greetings, farewells, shop ecc...) it belongs to\r\n",
        "      output_row=out_empty[:]\r\n",
        "      output_row[labels.index(docs_y[x])] =1\r\n",
        "      training.append(bag)\r\n",
        "      output.append(output_row)\r\n",
        "\r\n",
        "  training = np.array(training)\r\n",
        "  output = np.array(output)\r\n",
        "\r\n",
        "## MODEL DEFINITION:  \r\n",
        "\r\n",
        "\r\n",
        "  ## this defines the model expectation about the shape and size of the input\r\n",
        "  net = tflearn.input_data(shape=[None,len(training[0])])\r\n",
        "## here we define the layers of the neural network\r\n",
        "  net = tflearn.fully_connected(net,8)\r\n",
        "  net = tflearn.fully_connected(net,8)\r\n",
        "  net = tflearn.fully_connected(net,len(output[0]), activation='softmax')\r\n",
        "  net = tflearn.regression(net)\r\n",
        "\r\n",
        "  model = tflearn.DNN(net)\r\n",
        "\r\n",
        "model.fit(training,output, n_epoch=300, batch_size=8 ,show_metric=True)\r\n",
        "\r\n",
        "\r\n",
        "## one hot encoding\r\n",
        "def bag_of_words(s,words):\r\n",
        "    bag = [0 for _ in range(len(words))]\r\n",
        "    s_words = nltk.word_tokenize(s)\r\n",
        "    s_words = [stemmer.stem(word.lower()) for word in s_words]\r\n",
        "\r\n",
        "    for se in s_words:\r\n",
        "      for i,w in enumerate(words):\r\n",
        "        if w == se:\r\n",
        "          bag[i]=1\r\n",
        "    return np.array(bag)\r\n",
        "\r\n",
        "## CHATBOT FUNCTION:\r\n",
        "def chat():\r\n",
        "    print('Start talking with the bot (type quit to stop)')\r\n",
        "    while True:\r\n",
        "      inp = input('You:  ')\r\n",
        "      if inp.lower() == 'quit':\r\n",
        "        break \r\n",
        "      ## PREDICTING THE LABEL OF THE USER'S SENTENCE\r\n",
        "      ## remember the square brackets for the bag otherwyse we have dimensional problems\r\n",
        "      result = model.predict([bag_of_words(inp,words)])\r\n",
        "      result_index = np.argmax(result)\r\n",
        "      tag = labels[result_index]\r\n",
        "\r\n",
        "      ## defining a threshold for considering the answer \"valid\"\r\n",
        "      if result[:,result_index] > 0.6:\r\n",
        "        \r\n",
        "          #formulate an answer\r\n",
        "          for t in data['intents']:\r\n",
        "            if t['tag'] == tag:\r\n",
        "              answers = t['responses']\r\n",
        "          print(random.choice(answers))\r\n",
        "      else:\r\n",
        "          print(\"sorry, I didn't understand, can you repeat?\")\r\n",
        "\r\n",
        "chat()\r\n",
        "\r\n",
        "\r\n",
        "## The model has some issues like the fact that the model needs to be trained everytime,\r\n",
        "## this could be fixed saving the model in the repository and loading it every time, other problems\r\n",
        "## regard the limited amount of data disposable and the generality of the answers, aspects\r\n",
        "## that can be improved with some solutions that are beyond the scope of this application\r\n",
        "      \r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        " \r\n",
        "\r\n",
        "\r\n"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training Step: 2999  | total loss: \u001b[1m\u001b[32m0.14399\u001b[0m\u001b[0m | time: 0.041s\n",
            "| Adam | epoch: 300 | loss: 0.14399 - acc: 0.9677 -- iter: 72/75\n",
            "Training Step: 3000  | total loss: \u001b[1m\u001b[32m0.13311\u001b[0m\u001b[0m | time: 0.045s\n",
            "| Adam | epoch: 300 | loss: 0.13311 - acc: 0.9709 -- iter: 75/75\n",
            "--\n",
            "Start talking with the bot (type quit to stop)\n",
            "You:  hi\n",
            "8\n",
            "[[1.0356917e-08 1.2158005e-05 3.7071658e-03 3.4041204e-11 4.5084051e-04\n",
            "  1.1329299e-02 5.6722009e-05 2.1088002e-03 9.4344360e-01 2.6178097e-09\n",
            "  2.2615450e-09 2.5677268e-04 8.3220010e-07 9.9239540e-12 9.0788053e-09\n",
            "  1.1437112e-06 3.8632002e-02 1.3885749e-09 5.7926587e-07]]\n",
            "Good to see you again\n",
            "You:  hello\n",
            "8\n",
            "[[1.3556404e-07 7.4430434e-05 6.1411201e-03 1.1308838e-09 9.2729559e-04\n",
            "  1.6954066e-02 2.6885606e-04 6.0440199e-03 9.0662426e-01 6.1691665e-08\n",
            "  3.0627437e-08 1.1864341e-03 8.6378814e-06 5.8097549e-10 1.2311857e-07\n",
            "  6.5503368e-06 6.1755423e-02 4.4182148e-08 8.5415222e-06]]\n",
            "Hello, thanks for visiting\n",
            "You:  who are you\n",
            "5\n",
            "[[1.04002036e-04 3.05996888e-04 6.16628735e-04 1.03652686e-08\n",
            "  4.95162867e-02 9.31141615e-01 1.27486166e-09 2.76313611e-10\n",
            "  1.75685470e-03 3.66123931e-09 3.48079557e-05 3.65320690e-11\n",
            "  3.88463133e-14 1.01734119e-13 5.96296923e-06 1.65167749e-02\n",
            "  9.73488341e-07 1.00128495e-15 3.61458313e-13]]\n",
            "Hi I'm friday and i'm an AI created for chatting with humans\n",
            "You:  who am i?\n",
            "1\n",
            "[[1.68249628e-03 9.89241898e-01 1.79211729e-10 6.37824451e-06\n",
            "  6.03072579e-08 6.06073393e-03 8.52111296e-13 1.14679752e-10\n",
            "  2.26173488e-05 2.97846040e-03 1.47635194e-06 1.04527658e-07\n",
            "  7.29013811e-12 1.06746135e-07 1.57180207e-08 5.57704698e-06\n",
            "  9.01592134e-11 3.41235640e-09 1.22747167e-07]]\n",
            "Yes, you are a human\n",
            "You:  who is god\n",
            "0\n",
            "[[9.63089108e-01 7.67565472e-03 5.32925266e-13 7.97684886e-04\n",
            "  1.92673809e-07 1.83985801e-04 4.92471594e-19 4.42333257e-21\n",
            "  1.26515985e-12 2.20500260e-05 1.37979295e-02 9.36849149e-19\n",
            "  1.11199116e-21 1.30515224e-10 1.46581460e-05 1.44187389e-02\n",
            "  2.28438685e-18 1.25314124e-17 4.84435811e-17]]\n",
            "i'm not sure right now\n",
            "You:  tell me a joke\n",
            "9\n",
            "[[1.9608275e-03 2.1864049e-02 1.7346980e-17 5.3618234e-03 2.9682850e-14\n",
            "  2.7428559e-09 1.7684945e-16 3.7781223e-15 4.9828772e-13 9.5982844e-01\n",
            "  6.0118907e-07 4.2725537e-10 2.4543098e-12 1.0975068e-02 3.1853573e-09\n",
            "  5.1131215e-09 1.4530425e-17 7.8916091e-06 1.3049506e-06]]\n",
            "Did you hear oxygen went on a date with potassium? A: It went OK.\n",
            "You:  quit\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}